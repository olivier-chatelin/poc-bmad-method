---
stepsCompleted: [1, 2, 3, 4, 5]
inputDocuments:
  - '_bmad-output/analysis/brainstorming-session-2026-01-12.md'
date: 2026-01-12
author: Olivier
---

# Product Brief: Chatbot MCP Lab

## Executive Summary

Chatbot MCP Lab est un orchestrateur d'automatisations permettant aux d√©veloppeurs de tester et combiner rapidement des MCP (Model Context Protocol) locaux et distants avec diff√©rents LLMs (OpenAI GPT-4o, Claude). L'outil r√©sout le probl√®me de lenteur et complexit√© actuelle lors de l'exp√©rimentation MCP via des outils comme Claude Code ou GitHub Copilot, en permettant de brancher/d√©brancher des MCP en quelques clics via une interface Web.

Au-del√† de l'apprentissage personnel, ce projet sert d'esquisse pour une future feature professionnelle n√©cessitant l'impl√©mentation d'un bot avec MCP, offrant ainsi un double ROI: validation d'architecture et R&D pour besoins m√©tier.

---

## Core Vision

### Problem Statement

Les d√©veloppeurs voulant exp√©rimenter avec MCP font face √† un processus lent et technique. Tester diff√©rents MCP ou combinaisons (Notion+Teams+GitHub+Figma+Playwright) via des outils existants (Claude Code, GitHub Copilot) n√©cessite des manipulations r√©p√©titives, sans persistance de configuration, et sans capacit√© de comparaison rapide entre diff√©rentes combinaisons.

De plus, la documentation MCP √©tant r√©cente et instable, il est critique de pouvoir exp√©rimenter rapidement pour d√©couvrir ce qui fonctionne r√©ellement versus ce qui est th√©orique.

### Problem Impact

**Pour les d√©veloppeurs:**
- Apprentissage MCP ralenti par friction technique
- POCs MCP prennent trop de temps
- Impossibilit√© de tester rapidement des hypoth√®ses ("et si je combinais A+B+C?")
- Risque de perdre du temps sur des approches non viables

**Pour les organisations:**
- R&D MCP co√ªteuse en temps d√©veloppeur
- Difficile d'√©valuer la faisabilit√© de workflows MCP avant impl√©mentation production
- Pas d'outil pour valider architectures multi-LLM + MCP

### Why Existing Solutions Fall Short

**Claude Code / GitHub Copilot:**
- Pas con√ßus pour exp√©rimentation MCP rapide
- Aucune interface de gestion MCP (brancher/d√©brancher en clics)
- Pas de persistance de configurations MCP test√©es
- Impossibilit√© de comparer comportements entre LLMs sur m√™me MCP
- Pas d'observabilit√© sur pipeline complet (UI ‚Üí LLM ‚Üí MCP)

**Approches manuelles (code direct):**
- Trop lent: modification code, red√©marrage serveur √† chaque test
- Risque de code spaghetti lors d'exp√©rimentations rapides
- Pas de r√©utilisabilit√© entre POCs

**Aucune solution existante ne permet:**
- Configuration MCP par UI (pas fichiers)
- Switching LLM dynamique (optimisation co√ªts tokens)
- Orchestration multi-MCP en quelques clics
- Observabilit√© compl√®te pour debug et apprentissage

### Proposed Solution

Chatbot MCP Lab est une application Web locale permettant de:

**1. Gestion MCP Simplifi√©e**
- Brancher/d√©brancher MCP (locaux ou distants) via interface UI
- Configuration en clics, pas en code
- Catalogue de MCP test√©s avec m√©tadonn√©es (performance, compatibilit√©)

**2. Multi-LLM Intelligent**
- Switch entre OpenAI GPT-4o (tokens quasi-illimit√©s) et Claude (tokens limit√©s)
- Optimisation √©conomique automatique
- Comparaison comportements LLMs sur m√™mes workflows

**3. Orchestration Workflow**
- Tester combinaisons MCP (ex: Notion+Teams+GitHub+Figma+Playwright)
- Workflows pr√©-configur√©s r√©utilisables
- Discovery de patterns efficaces multi-MCP

**4. Observabilit√© D√©veloppeur**
- Tra√ßage complet: UI ‚Üí Adapter ‚Üí LLM ‚Üí MCP
- Debug visuel des transformations Tools‚ÜíFunctions
- Logs et m√©triques pour optimisation

**5. Architecture Propre (Validation BMAD)**
- D√©velopp√© avec m√©thode BMAD pour √©viter code spaghetti
- Patterns r√©utilisables (Strategy Pattern adapter, Canonical Format mapping)
- Foundation solide pour projets professionnels futurs

### Key Differentiators

**1. UI-First pour D√©veloppeurs**
- Premi√®re solution permettant configuration MCP sans toucher au code
- Exp√©rience optimis√©e pour POCs rapides (10 POCs/jour vs 1/semaine)

**2. Multi-LLM √âconomique**
- Seul outil permettant switch LLM dynamique pour optimisation co√ªts tokens
- Insights uniques sur forces/faiblesses de chaque LLM avec MCP

**3. Laboratoire d'Orchestration**
- Focus sur combinaisons MCP (pas MCP individuels)
- Discovery de workflows cross-platform (Design‚ÜíCode‚ÜíDeploy automatis√©s)

**4. Observabilit√© Compl√®te**
- Tra√ßabilit√© totale pour apprentissage et debug
- Visualisation transformations MCP‚ÜíLLM

**5. Production-Ready Architecture**
- Pas un hack - architecture solide r√©utilisable en production
- Esquisse valid√©e pour projets professionnels
- Patterns √©prouv√©s, pas exp√©rimentations jetables

**6. Timing Parfait**
- MCP r√©cent = peu d'experts, opportunit√© devenir thought leader
- Besoin professionnel futur = double ROI imm√©diat

---

## Target Users

### Primary Users

**Persona: Alex - D√©veloppeur Full-Stack Explorateur MCP**

**Contexte:**
- D√©veloppeur full-stack (3-7 ans exp√©rience)
- Licences OpenAI + Claude disponibles
- Besoin professionnel imminent: impl√©menter bot avec MCP
- Utilise actuellement Claude Code ou GitHub Copilot

**Probl√®me Exp√©riment√©:**
- Teste MCP via outils g√©n√©ralistes (Claude Code) - c'est **long**
- Chaque test MCP n√©cessite reformulation, pas de persistance config
- Impossible de tester rapidement combinaisons (Notion+Teams+GitHub...)
- Documentation MCP r√©cente/instable = besoin d'exp√©rimenter pour apprendre

**Motivations:**
- Ma√Ætriser MCP avant projet pro (avantage comp√©titif)
- √âviter code spaghetti lors d'exp√©rimentations
- Valider architectures avant impl√©mentation production
- Devenir expert d'un domaine √©mergent

**Succ√®s Pour Alex:**
"Je branche GitHub MCP en 2 clics, je teste avec GPT-4o, puis je switch vers Claude pour comparer - total 5 minutes. J'ai une trace compl√®te de ce qui s'est pass√©. Demain j'ajoute Notion MCP et je teste la combo GitHub+Notion. En une semaine, j'ai valid√© 3 architectures diff√©rentes et je sais laquelle utiliser au boulot."

### Secondary Users

**N/A pour MVP** - Utilisateur unique.

**Pour versions futures:**
- **Dev Teams**: √âquipes voulant standardiser workflows MCP
- **Engineering Managers**: √âvaluer faisabilit√© MCP avant investir
- **MCP Developers**: Tester leurs propres MCP contre diff√©rents LLMs

### User Journey

**Discovery (Semaine 0):**
- Alex a un projet pro n√©cessitant bot MCP dans 2-3 mois
- Cherche outil pour exp√©rimenter rapidement
- Trouve Chatbot MCP Lab (GitHub, bouche-√†-oreille dev)

**Onboarding (Jour 1 - 6.5h):**
- Clone repo, suit README
- Spike #1: Teste Strategy Pattern adapter (4h)
- Premier MCP GitHub connect√© via UI
- **Aha moment:** "J'ai branch√© un MCP sans toucher au code!"

**Core Usage (Semaines 1-4):**
- S1: Spikes architecturaux, d√©cisions tech stack
- S2: MVP mono-LLM (GPT-4o) + GitHub MCP fonctionnel
- S3: **Utilise quotidiennement** pour POCs
- S4: D√©cision go/no-go multi-LLM

**Success Moment (Semaine 3):**
Alex teste combo GitHub+Playwright en 10 minutes. L'observabilit√© UI montre exactement le pipeline complet. Il d√©couvre un pattern MCP qu'il pourra r√©utiliser au boulot. "C'est exactement ce que je cherchais!"

**Long-term (Mois 2-3):**
- Lab devient indispensable
- 10+ POCs MCP test√©s
- Architecture valid√©e pour projet pro
- Alex partage learnings en interne, devient r√©f√©rent MCP

---

## Success Metrics

### User Success Metrics

**Vitesse d'Exp√©rimentation MCP:**
- **M√©trique:** Temps moyen pour tester un nouveau MCP
- **Objectif:** < 5 minutes (vs ~30-60min avec Claude Code)
- **Mesure:** Tracking UI temps entre "add MCP" et "premi√®re r√©ponse LLM"

**Fr√©quence POCs:**
- **M√©trique:** Nombre de POCs MCP r√©alis√©s par semaine
- **Objectif:** 10 POCs/jour vs 1 POC/semaine (70x improvement)
- **Mesure:** Compteur configurations MCP test√©es

**Adoption Quotidienne:**
- **M√©trique:** Utilisation quotidienne du lab
- **Objectif:** 3+ utilisations/semaine minimum en Semaine 3
- **Mesure:** Logs utilisation, sessions actives

**Insights MCP D√©couverts:**
- **M√©trique:** D√©couvertes inattendues sur MCP ou combinaisons
- **Objectif:** 1+ insight majeur par semaine
- **Mesure:** Documentation learnings

**Succ√®s Architecture:**
- **M√©trique:** Code reste propre (pas spaghetti) apr√®s 4 semaines
- **Objectif:** Architecture respecte ADRs, maintenabilit√© haute
- **Mesure:** Code review S4, dette technique

### Business Objectives

**Objectif 1: Validation BMAD (Meta-Objectif)**
- Timeline: 4 semaines
- Crit√®re: Code propre sans spaghetti, ADRs suivis, architecture solide
- Impact: M√©thode BMAD valid√©e pour projets complexes

**Objectif 2: R&D Projet Professionnel**
- Timeline: 2-3 mois
- Crit√®re: Architecture valid√©e et r√©utilisable pour bot professionnel
- Impact: √âconomie temps R&D, esquisse production-ready

**Objectif 3: Ma√Ætrise MCP**
- Timeline: 3 mois
- Crit√®re: 10+ MCP test√©s, patterns document√©s, expert reconnu
- Impact: Avantage comp√©titif professionnel, thought leadership

**Objectif 4: Tooling Personnel**
- Timeline: 3 mois
- Crit√®re: Lab utilis√© quotidiennement, indispensable au workflow
- Impact: Productivit√© d√©veloppement x10

### Key Performance Indicators

**KPIs Semaine 4 (MVP Validation):**

‚úÖ **Architecture KPIs:**
- Strategy Pattern Adapter propre (code review clean)
- 1 MCP GitHub fonctionnel GPT-4o (tests passent)
- Observability debug fonctionnelle (trac√©s UI)
- Z√©ro code spaghetti (revue architecture)

‚úÖ **Usage KPIs:**
- Lab utilis√© 3√ó minimum S3 (logs)
- 1+ POC MCP < 10min (mesure temps)
- 1+ insight MCP inattendu (documentation)

‚úÖ **BMAD Validation KPIs:**
- Tous ADRs document√©s et suivis
- Pas code vite fait non test√© (coverage > 70%)
- Confiance architecture scale (Go multi-LLM)

**KPIs 3 Mois (Production Maturity):**

üìà **Adoption:**
- Lab 5+ jours/semaine
- 10+ MCP test√©s
- 3+ combos workflow valid√©s

üìà **Learning:**
- 5+ patterns document√©s
- 3+ ADRs architecturaux
- 1+ pr√©sentation interne

üìà **Business Impact:**
- Architecture r√©utilis√©e projet pro
- 10h+/semaine gagn√©es
- Co√ªts tokens optimis√©s

---

## MVP Scope

### Core Features (Semaines 1-4)

**Phase 1 - MVP Ruthless (S1-S2):**

**1. Adapter Multi-LLM (GPT-4o Only pour MVP)**
- Strategy Pattern adapter impl√©ment√©
- Support GPT-4o uniquement (pas Claude en MVP)
- API key management simple (env file)
- **Rationale:** Valider architecture sans complexit√© multi-LLM

**2. Int√©gration MCP Unique (GitHub Only)**
- Connexion 1 MCP distant (GitHub)
- Configuration via UI basique (pas fichier config)
- Test workflow: UI ‚Üí Adapter ‚Üí GPT-4o ‚Üí GitHub MCP
- **Rationale:** Prouver concept end-to-end avec MCP le plus simple

**3. Canonical Format Transformation**
- Format interm√©diaire unifi√© d√©fini
- Mapper MCP Tools ‚Üí Canonical ‚Üí GPT-4o Functions
- Gestion erreurs transformation basique
- **Rationale:** Foundation pour multi-LLM futur

**4. Observability-First**
- Logging complet (UI ‚Üí Adapter ‚Üí LLM ‚Üí MCP)
- Tracing requ√™tes avec timestamps
- UI debug affichant logs/traces en temps r√©el
- **Rationale:** Essentiel pour apprentissage MCP et debug

**5. UI Web Minimale**
- Interface brancher/d√©brancher GitHub MCP (clics)
- S√©lection LLM (GPT-4o pour MVP)
- Chat interface basique
- Vue logs/observability
- **Rationale:** Juste assez UI pour valider concept "configuration par clics"

### Out of Scope for MVP

**‚ùå Multi-LLM (Claude, Gemini)**
- D√©cision: Mono-LLM (GPT-4o) seulement
- Pourquoi defer: Complexit√© adapter, focus MCP d'abord
- Quand: Semaine 4 (go/no-go apr√®s S3 validation)

**‚ùå Multiple MCP**
- D√©cision: 1 seul MCP (GitHub)
- Pourquoi defer: Valider pattern avec 1 MCP avant scale
- Quand: Post-S4 si Go

**‚ùå Orchestration Combos MCP**
- D√©cision: Pas combos (Notion+Teams+GitHub) en MVP
- Pourquoi defer: Complexit√© exponentielle
- Quand: Phase 3 (S5-S8)

**‚ùå Persistence Conversations**
- D√©cision: Pas DB, sessions in-memory
- Pourquoi defer: Pas critique validation POC
- Quand: V2 si adoption quotidienne

**‚ùå Advanced UI Features**
- Pas visualization transformations MCP‚ÜíLLM
- Pas multi-keys management
- Pas export conversations
- Quand: Post-MVP si adoption

**‚ùå MCP Locaux**
- D√©cision: MCP distants seulement
- Pourquoi defer: Moins complexit√© setup
- Quand: V2 si besoin

**‚ùå Tests Automatis√©s Complets**
- D√©cision: Tests manuels MVP, quelques tests unitaires
- Quand: Phase 4 (S9-12) contract testing

### MVP Success Criteria (Validation S4)

**Go/No-Go Decision Criteria (Fin Semaine 4):**

‚úÖ **GO si:**
- Strategy Pattern adapter propre, maintenable
- GitHub MCP fonctionne avec GPT-4o
- Observability permet debug/learning
- Utilis√© 3√ó durant S3 minimum
- 1+ POC MCP < 10min
- Confiance architecture peut scale
- BMAD a emp√™ch√© spaghetti

‚ùå **NO-GO si:**
- Code spaghetti apr√®s S2
- Adapter trop complexe
- GitHub MCP non fiable
- Aucune utilisation S3
- Paralysie planning

**Si GO:** Multi-LLM (Claude en S5-S8)
**Si NO-GO:** It√©rer mono-LLM ou pivoter

### Future Vision

**Phase 2 - Multi-LLM (Si GO, S5-S8):**
- Ajouter Claude (valider Strategy Pattern scale)
- Switching LLM dynamique via UI
- Comparaison GPT-4o vs Claude
- Optimisation co√ªts tokens

**Phase 3 - Orchestration MCP (S9-S12+):**
- 3 combos magiques valid√©s
- Workflows pr√©-configur√©s r√©utilisables
- Catalogue MCP test√©s

**Phase 4 - Production Maturity (Mois 3+):**
- Architecture r√©utilis√©e projet pro
- MCP locaux support√©s
- Persistence conversations (DB)
- Tests contract-based complets
- Visualization transformations

**Vision Long-Terme (6-12 mois):**
- Plateforme open-source d√©veloppeurs
- Biblioth√®que patterns MCP document√©s
- Support Gemini, Llama, autres LLMs
- Thought leadership MCP
- Communaut√© contributeurs
